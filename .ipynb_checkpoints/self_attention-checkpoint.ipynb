{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce807224",
   "metadata": {},
   "source": [
    "**Self Attention, what it is ?**\n",
    "- As the name states, attention which means empowering something to focus on important parts of the something. So in deep learning we focus more on input data. At first attention mechanism was introduced for RNN's for handling longer sequence to sequence information, but the problem arise with the text having a context, so for like ...if we do some kind of text translation, where the context of each word of occuring in a sentence. So that's why we needed context vector, which contains all the necessary information of the text. \n",
    "\n",
    "1. **Multiplicative Attention (Dot-Product Attention)** : so in here, we do the simple dot product of query and key vectors. The formula is given by :\n",
    "\n",
    "    Attention(Q, K, V) = softmax(QK^T)V\n",
    "\n",
    "    But the problem arises doing this, as the dimensions of the vector increases, the variance of dot product also increases. So when passed through the softmax function, can lead to higher probabilites being assigned to higher values and lower probabilities to lower values. To overcome this we do scaled dot product mechanism. \n",
    "2. **Scaled Dot-Product Attention** : So this is the popular self-attention mechanism. It Computes attention score taking dot product of query, key and value vector. And it scales the dot product by the square root of the dimension of the key vectors. \n",
    "\n",
    "Attention(Q,K,V)=softmax(QKT/√dk​)V\n",
    "\n",
    "So the purpose of doing so is to stablizing the training process and prevent the dot product to become too large or too small.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ecbb7be-0c8d-4979-a02b-8dd09a3bdd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA: True\n",
      "Device name: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "# implemting self-attention in transformers\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f810c5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8360188  0.11314284 0.05083836]\n"
     ]
    }
   ],
   "source": [
    "## simple softmax function \n",
    "import numpy as np\n",
    "\n",
    "def softmax_function(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "scores = [3.0, 1.0, 0.2] ## so that's why we do scaling to get the probability \n",
    "print(softmax_function(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6ebe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b3740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e435f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
