{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing Multi-head latent Attention \n",
    "\n",
    "Breakthrough into Multi-head latent attention, it was the game changer because it improves the computioanl cost of training.\n",
    "- The usual multi head attention has drawback as it require a quite large amount of memory to store keys(k) and values(v) during inference for every token it has seen so far\n",
    " this is called Key-Value (KV) cache. This size of KV cache grows linearly with the sequence length.\n",
    "- And here it comes Multi-Head latent attention, it's a new attention mechanism design to solve the memory KV cache issue problem. It acieves this by compressing the key and values into smaller, shared representation called a latent vector. This reduces the size.\n",
    "\n",
    "MLA introduces two innovations :\n",
    "\n",
    "1. **Low-Rank Key-Value Compression**: so the idea behind is that so compress the key and values into a smaller representation. Instead of storing full keys and values, MLA compress them into a latent vector. This latent vector is much smaller than the orignal keys and values, significantly reducing memory usage.\n",
    "$$\\text{Let } K \\in \\mathbb{R}^{n \\times d} \\text{ and } V \\in \\mathbb{R}^{n \\times d} \\text{ be the original key and value matrices.}\\\\\n",
    "\\text{We decompose them into low-rank factors:}$$\n",
    "\n",
    "$$K \\approx K_A K_B = \\begin{bmatrix} k_1^T \\\\ k_2^T \\\\ \\vdots \\\\ k_n^T \\end{bmatrix} \\text{ where } K_A \\in \\mathbb{R}^{n \\times r}, K_B \\in \\mathbb{R}^{r \\times d}$$\n",
    "\n",
    "$$V \\approx V_A V_B = \\begin{bmatrix} v_1^T \\\\ v_2^T \\\\ \\vdots \\\\ v_n^T \\end{bmatrix} \\text{ where } V_A \\in \\mathbb{R}^{n \\times r}, V_B \\in \\mathbb{R}^{r \\times d}$$\n",
    "\n",
    "$$\\text{where } r \\ll d \\text{ is the rank, achieving compression ratio of } \\frac{2rd}{d(n+d)} \\text{ for large } n$$\n",
    "\n",
    "$$\\text{Standard attention: } \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V$$\n",
    "\n",
    "$$\\text{Low-rank attention: } \\text{Attention}(Q, K_A, K_B, V_A, V_B) = \\text{softmax}\\left(\\frac{Q(K_A K_B)^T}{\\sqrt{d}}\\right)(V_A V_B)$$\n",
    "\n",
    "$$= \\text{softmax}\\left(\\frac{QK_B^T K_A^T}{\\sqrt{d}}\\right)V_A V_B$$\n",
    "\n",
    "$$\\text{Original memory: } \\mathcal{O}(nd) \\text{ for both K and V}$$\n",
    "\n",
    "$$\\text{Low-rank memory: } \\mathcal{O}(nr + rd) = \\mathcal{O}(r(n + d))$$\n",
    "\n",
    "\n",
    "$$\\text{At inference step } t, \\text{ given new query } q_t \\in \\mathbb{R}^d \\text{ and cached low-rank KV:}$$\n",
    "\n",
    "$$K_{\\text{cache}} = K_A K_B \\text{ where } K_A \\in \\mathbb{R}^{(t-1) \\times r}, K_B \\in \\mathbb{R}^{r \\times d}$$\n",
    "\n",
    "$$V_{\\text{cache}} = V_A V_B \\text{ where } V_A \\in \\mathbb{R}^{(t-1) \\times r}, V_B \\in \\mathbb{R}^{r \\times d}$$\n",
    "\n",
    "$$\\text{Compute new key-value pair for position } t:$$\n",
    "\n",
    "$$k_t = \\text{Linear}_K(x_t), \\quad v_t = \\text{Linear}_V(x_t)$$\n",
    "\n",
    "$$\\text{Decompose into low-rank factors:}$$\n",
    "\n",
    "$$k_t = k_{t,A} k_{t,B} \\text{ where } k_{t,A} \\in \\mathbb{R}^{1 \\times r}, k_{t,B} \\in \\mathbb{R}^{r \\times d}$$\n",
    "\n",
    "$$v_t = v_{t,A} v_{t,B} \\text{ where } v_{t,A} \\in \\mathbb{R}^{1 \\times r}, v_{t,B} \\in \\mathbb{R}^{r \\times d}$$\n",
    "\n",
    "$$\\text{Update cached factors by concatenation:}$$\n",
    "\n",
    "$$K_A^{(t)} = \\begin{bmatrix} K_A^{(t-1)} \\\\ k_{t,A} \\end{bmatrix} \\in \\mathbb{R}^{t \\times r}$$\n",
    "\n",
    "$$K_B^{(t)} = K_B^{(t-1)} = k_{t,B} \\in \\mathbb{R}^{r \\times d} \\quad \\text{(shared across positions)}$$\n",
    "\n",
    "$$V_A^{(t)} = \\begin{bmatrix} V_A^{(t-1)} \\\\ v_{t,A} \\end{bmatrix} \\in \\mathbb{R}^{t \\times r}$$\n",
    "\n",
    "$$V_B^{(t)} = V_B^{(t-1)} = v_{t,B} \\in \\mathbb{R}^{r \\times d} \\quad \\text{(shared across positions)}$$\n",
    "\n",
    "$$\\text{Compute attention scores efficiently:}$$\n",
    "\n",
    "$$\\text{scores}_t = \\frac{q_t (K_A^{(t)} K_B^{(t)})^T}{\\sqrt{d}} = \\frac{q_t (K_B^{(t)})^T (K_A^{(t)})^T}{\\sqrt{d}}$$\n",
    "\n",
    "$$= \\frac{(q_t (K_B^{(t)})^T) (K_A^{(t)})^T}{\\sqrt{d}} \\in \\mathbb{R}^{1 \\times t}$$\n",
    "\n",
    "$$\\text{where } q_t (K_B^{(t)})^T \\in \\mathbb{R}^{1 \\times r} \\text{ is computed once}$$\n",
    "\n",
    "$$\\alpha_t = \\text{softmax}(\\text{scores}_t) \\in \\mathbb{R}^{1 \\times t}$$\n",
    "\n",
    "$$\\text{output}_t = \\alpha_t (V_A^{(t)} V_B^{(t)}) = (\\alpha_t V_A^{(t)}) V_B^{(t)}$$\n",
    "\n",
    "$$= \\sum_{i=1}^{t} \\alpha_{t,i} \\cdot (v_{i,A} v_{i,B}) = \\left(\\sum_{i=1}^{t} \\alpha_{t,i} v_{i,A}\\right) V_B^{(t)}$$\n",
    "\n",
    "\n",
    "2. **Decoupled Rotary position Embedding**: technique to encode the position into tokens of sequence. However it normal RoPE face challenge because when using low-rank compression, the position information get mixed into the compressed key and values, making it hard to reuse them efficiently during inference. \n",
    "- To efficiently use while maintaining memory efficiency, MLA uses a decoupled RoPE strategy. This introduces additional multi-head queries and a shared key to encoder RoPE.\n",
    "$$\\text{Standard RoPE applies rotation to both queries and keys:}$$\n",
    "\n",
    "$$q_m = \\text{RoPE}(q_m, m), \\quad k_m = \\text{RoPE}(k_m, m)$$\n",
    "\n",
    "$$\\text{In MLA, decouple position encoding from compressed representations:}$$\n",
    "\n",
    "$$\\text{Shared key: } k_m^{\\text{shared}} = \\text{RoPE}(W_k^{\\text{shared}} x_m, m) \\in \\mathbb{R}^{d_k}$$\n",
    "\n",
    "$$\\text{Multi-head queries: } q_{m,h} = \\text{RoPE}(W_{q,h} x_m, m) \\in \\mathbb{R}^{d_k} \\text{ for head } h$$\n",
    "\n",
    "$$\\text{Low-rank KV (position-free): } k_m^{\\text{LR}} = W_k^{\\text{LR}} x_m, \\quad v_m^{\\text{LR}} = W_v^{\\text{LR}} x_m$$\n",
    "\n",
    "$$\\text{Standard RoPE applies rotation to both queries and keys:}$$\n",
    "\n",
    "$$q_m = \\text{RoPE}(q_m, m), \\quad k_m = \\text{RoPE}(k_m, m)$$\n",
    "\n",
    "$$\\text{Attention combines position-aware and compressed components:}$$\n",
    "\n",
    "$$\\text{Attention}_h(m) = \\alpha_{m,h}^{\\text{pos}} \\cdot k_m^{\\text{shared}} + \\alpha_{m,h}^{\\text{content}} \\cdot (k_m^{\\text{LR}} v_m^{\\text{LR}})$$\n",
    "\n",
    "$$\\text{where: } \\alpha_{m,h}^{\\text{pos}} = \\text{softmax}\\left(\\frac{q_{m,h}^T k_{\\cdot}^{\\text{shared}}}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "$$\\alpha_{m,h}^{\\text{content}} = \\text{softmax}\\left(\\frac{q_{m,h}^T k_{\\cdot}^{\\text{LR}}}{\\sqrt{d_k}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So one of the key optimization in MLA is the absorption of weight matrices. This allow the model to allow to avoid explicitly reconstructing the keys and value during inference, saving both computation time and memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import math \n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    '''implementing decoupled RoPE'''\n",
    "    def __init__(self, d_model, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base \n",
    "\n",
    "        # inverse frequency and register as a buffer to check and save how fast each dimensions rotate \n",
    "        inverse_freq = 1.0 / (self.base **(torch.arange(0, self.d_model, 2).float().to(device)/self.d_model))\n",
    "        self.register_buffer(\"inverse_freq\", inverse_freq, persistent = False)\n",
    "\n",
    "        self.cache(seq_len = max_seq_len, device=device, dtype =torch.float32)\n",
    "    \n",
    "    def cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inverse_freq.dtype)\n",
    "\n",
    "        frequencies = torch.einsum(\"i,j->ij\", t, self.inverse_freq)\n",
    "        embedding = torch.cat((frequencies, frequencies), dim=-1)\n",
    "        self.register_buffer(\"cosine_cached\", embedding.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sine_cached\", embedding.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self.cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "        return (\n",
    "            self.cosine_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sine_cached[:seq_len].to(dtype=x.dtype),\n",
    "\n",
    "        )\n",
    "\n",
    "def rotate_half(x):\n",
    "    '''rotate half of the hidde dim from the input'''\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary(x, cos, sin, position_ids):\n",
    "    # cos and sin have shape [seq_len, dim]\n",
    "    # position_ids have shape [batch, seq_len]\n",
    "    cos = cos[position_ids].unsqueeze(2) # [bs, seq_len, 1, dim]\n",
    "    sin = sin[position_ids].unsqueeze(2) # [bs, seq_len, 1, dim]\n",
    "    # x has shape [bs, seq_len, n_heads, dim]\n",
    "    x_embed = (x * cos) + (rotate_half(x) * sin)\n",
    "    return x_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLatent(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, kv_latent_dim):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.kv_latent_dim = kv_latent_dim\n",
    "        self.dh = d_model//n_heads\n",
    "        self.rotary_emb = RoPE(self.kv_latent_dim)\n",
    "\n",
    "        # projection layer\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # q matrix\n",
    "        self.w_dkv = nn.Linear(d_model, kv_latent_dim, bias=False) # compressed kv \n",
    "        self.w_uk = nn.Linear(kv_latent_dim, d_model, bias=False) # takes compress intermediate key representation vector as input\n",
    "        self.w_uv = nn.Linear(kv_latent_dim, d_model, bias=False) # takes compress intermediate value representation vector as input\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # output layer\n",
    "        self.ln = nn.LayerNorm(kv_latent_dim)\n",
    "\n",
    "        self.register_buffer('absorbed_k', None) # have information about w_q @ w_uk\n",
    "\n",
    "    def forward(self, x, kv_cache=None, past_length=0):\n",
    "        B,S,D = x.size() # batch, sequence_lenth, dimmension\n",
    "\n",
    "        # calculate absorbed_k w_q @ w_uk\n",
    "        if self.absorbed_k is None:\n",
    "            absorbed = torch.matmul(self.w_q.weight, self.w_uk.weight)\n",
    "            self.absorbed_k = absorbed.view(self.n_heads, self.dh, self.kv_latent_dim) # reshaping\n",
    "\n",
    "        # compress input into latent kv \n",
    "        new_c_kv = self.ln(self.w_dkv(x))\n",
    "\n",
    "        if kv_cache is None:\n",
    "            c_kv = new_c_kv\n",
    "        else:\n",
    "            c_kv = torch.cat([kv_cache, new_c_kv], dim=1)\n",
    "\n",
    "        s_full = c_kv.size(1)\n",
    "\n",
    "        # Decompress V to full d_model and split into heads\n",
    "        v_full = self.w_uv(c_kv)\n",
    "        v = v_full.view(B,s_full,self.n_heads, self.dh).transpose(1,2)\n",
    "\n",
    "        # Use input x directly (since W_q is abosorbed)\n",
    "        q = x.view(B,S, self.n_heads, self.dh)\n",
    "\n",
    "        # project q into latent space\n",
    "        q_latent = torch.einsum('bshd, hdl->bshl', q, self.absorbed_k)\n",
    "\n",
    "        # get cos, sin\n",
    "        cos, sin = self.rotary_emb(x=q_latent, seq_len=s_full)\n",
    "\n",
    "        # apply rotary positional \n",
    "        query_pos = torch.arange(past_length, past_length+S, device=x.device).view(1, S)\n",
    "        q_rotate = apply_rotary(q_latent, cos, sin, query_pos)  \n",
    "        q_rotate = q_rotate.transpose(1, 2) # shape (B, n_heads, S, kv_latent_dim)\n",
    "\n",
    "        # Apply RoPE to keys (from c_kv) based on their positions\n",
    "        key_pos = torch.arange(0, s_full, device=x.device).view(1, s_full)\n",
    "        k_rotate = apply_rotary(c_kv.unsqueeze(2), cos, sin, key_pos) # shape (B, S_full, 1, kv_latent_dim)\n",
    "        k_rotate = k_rotate.transpose(1,2) # shape (B, 1, S_full, kv_latent_dim)\n",
    "\n",
    "        # compute attention score \n",
    "        # Compute scaled dot-product attention Q @ K.T.\n",
    "        # Shapes: (B, n_heads, S, D) @ (B, 1, D, S_full) -> (B, n_heads, S, S_full)\n",
    "        # The '1' head in K is broadcast to match the 'n_heads' in Q (Multi-Query Attention).\n",
    "        # The transpose(2,3) on K aligns the dimensions for the dot product.\n",
    "        attention_score = torch.matmul(q_rotate, k_rotate.transpose(2,3)) / math.sqrt(self.kv_latent_dim)\n",
    "        mask = torch.tril(torch.ones((S, s_full), device = x.device), diagonal=past_length) # casual masking\n",
    "        attention_score = attention_score.masked_fill(mask.view(1,1,S,s_full)==0, float('-inf'))\n",
    "        attention_weights = F.softmax(attention_score, dim=1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "        output = output.transpose(1,2).contiguous().view(B,S,D)\n",
    "\n",
    "        return self.w_o(output), c_kv # final output + updated latent cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 5, 512]), Cache shape:torch.Size([1, 5, 256])\n",
      "Memory: Standard KV Cache = 20.0 KB, Latent Cache = 5.0 KB, Reduction = 4.0x\n"
     ]
    }
   ],
   "source": [
    "# testing the memory \n",
    "def demo():\n",
    "    \n",
    "    model = MultiHeadLatent(d_model=512,n_heads=8,kv_latent_dim=256).to(device)\n",
    "    \n",
    "    x = torch.randn(1,5,512).to(device)\n",
    "\n",
    "    out, cache = model(x)\n",
    "    print(f\"Output shape: {out.shape}, Cache shape:{cache.shape}\")\n",
    "\n",
    "    B, S, D = x.shape\n",
    "    latent_dim = cache.shape[-1]\n",
    "    std_size = B * 2 * S * D * 4 / 1024\n",
    "    latent_size = B * S * latent_dim * 4 / 1024\n",
    "    print(f\"Memory: Standard KV Cache = {std_size:.1f} KB, Latent Cache = {latent_size:.1f} KB, Reduction = {std_size/latent_size:.1f}x\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting KV Cache Growth Demo ---\n",
      "Step 0: Initial prompt processed.\n",
      "        Input shape: (50 tokens)\n",
      "        Cache shape: torch.Size([1, 50, 4]) -> (Batch, Sequence, LatentDim)\n",
      "\n",
      "Step 1: Generating one new token...\n",
      "        Input shape: (1 token)\n",
      "        Cache shape: torch.Size([1, 51, 4]) -> Sequence length grew by 1!\n",
      "\n",
      "Step 2: Generating one new token...\n",
      "        Input shape: (1 token)\n",
      "        Cache shape: torch.Size([1, 52, 4]) -> Sequence length grew by 1!\n",
      "\n",
      "Step 3: Generating one new token...\n",
      "        Input shape: (1 token)\n",
      "        Cache shape: torch.Size([1, 53, 4]) -> Sequence length grew by 1!\n",
      "\n",
      "Step 4: Generating one new token...\n",
      "        Input shape: (1 token)\n",
      "        Cache shape: torch.Size([1, 54, 4]) -> Sequence length grew by 1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demo_kv_cache_growth(num_initial_tokens=5, num_new_tokens=3):\n",
    "  \"\"\"Demonstrates the growth of the KV cache during autoregressive decoding.\"\"\"\n",
    "  print(f\"--- Starting KV Cache Growth Demo ---\")\n",
    "  torch.manual_seed(0)\n",
    "\n",
    "  model = MultiHeadLatent(d_model=8, n_heads=2, kv_latent_dim=4).to(device)\n",
    "\n",
    "  # Move the input tensor to the device\n",
    "  x = torch.randn(1, num_initial_tokens, 8).to(device)\n",
    "  \n",
    "  # The first pass has no past, so kv_cache is None and past_length is 0\n",
    "  out, cache = model(x)\n",
    "  print(f\"Step 0: Initial prompt processed.\")\n",
    "  print(f\"        Input shape: ({num_initial_tokens} tokens)\")\n",
    "  print(f\"        Cache shape: {cache.shape} -> (Batch, Sequence, LatentDim)\\n\")\n",
    "\n",
    "  # Step 2: Incrementally append new tokens, one at a time\n",
    "  for step in range(1, num_new_tokens + 1):\n",
    "    # The past context is now the cache from the previous step\n",
    "    past_context = cache\n",
    "    \n",
    "    #  Move the new token tensor to the device\n",
    "    new_token = torch.randn(1, 1, 8).to(device)\n",
    "    \n",
    "    # Pass the new token and the existing cache to the model\n",
    "    out, cache = model(new_token, kv_cache=past_context, past_length=past_context.shape[1])\n",
    "    \n",
    "    print(f\"Step {step}: Generating one new token...\")\n",
    "    print(f\"        Input shape: (1 token)\")\n",
    "    print(f\"        Cache shape: {cache.shape} -> Sequence length grew by 1!\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_kv_cache_growth(num_initial_tokens=50, num_new_tokens=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
